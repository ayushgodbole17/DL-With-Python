{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJYn1rFmDfoq",
        "outputId": "71b65a88-3a8a-4277-d255-41a5a9bbe38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-29 06:37:26--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  16.8MB/s    in 8.1s    \n",
            "\n",
            "2022-07-29 06:37:35 (9.93 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow import keras\n",
        "dataset = keras.utils.text_dataset_from_directory(\n",
        " directory=\"aclImdb\", label_mode=None, batch_size=256)\n",
        "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \")) #Replace all <br /> tags, which occur in some reviews, with blank sapce"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecMJhZFHDkmQ",
        "outputId": "1468aad2-d6e1-4d3f-ebf9-cc4eafb38700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100006 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.layers as layers"
      ],
      "metadata": {
        "id": "HDI8qLVlEIfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Cjtoy0YVqZ7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "sequence_length = 100\n",
        "vocab_size = 15000   #Considering only top 15000 words, everything else treated as [UNK]\n",
        "text_vectorization = TextVectorization(\n",
        " max_tokens=vocab_size, \n",
        " output_mode=\"int\",    # We want to return integer word index sequences\n",
        " output_sequence_length=sequence_length,  #we work with inputs and targets of length sequence_length, which is equal to 100. \n",
        " #in reality, it's more like 99, since we offset targets by 1\n",
        ")\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "7i7Tow6CESFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_lm_dataset(text_batch):\n",
        " vectorized_sequences = text_vectorization(text_batch) #converting batch of strings to batch of integers\n",
        " x = vectorized_sequences[:, :-1]   #Create inputs by cutting off last word\n",
        " y = vectorized_sequences[:, 1:]    #Create targets by offsetting by 1\n",
        " return x, y\n",
        "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "LmYqWt2yEstW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "\n",
        " def __init__(self, sequence_length, input_dim, output_dim, **kwargs): \n",
        "  super().__init__(**kwargs)\n",
        "  self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
        "  self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim) \n",
        "  self.sequence_length = sequence_length\n",
        "  self.input_dim = input_dim\n",
        "  self.output_dim = output_dim\n",
        "\n",
        " def call(self, inputs):\n",
        "  length = tf.shape(inputs)[-1]\n",
        "  positions = tf.range(start=0, limit=length, delta=1)\n",
        "  embedded_tokens = self.token_embeddings(inputs)\n",
        "  embedded_positions = self.position_embeddings(positions)\n",
        "  return embedded_tokens + embedded_positions \n",
        "\n",
        " def compute_mask(self, inputs, mask=None): \n",
        "  return tf.math.not_equal(inputs, 0) \n",
        "\n",
        " def get_config(self): \n",
        "  config = super().get_config()\n",
        "  config.update({\"output_dim\": self.output_dim,\"sequence_length\": self.sequence_length, \"input_dim\": self.input_dim})\n",
        "  return config\n",
        "\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.attention2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.denseProj = keras.Sequential([layers.Dense(dense_dim, activation='relu'), layers.Dense(embed_dim), ])\n",
        "    self.layernorm1 = layers.LayerNormalization()\n",
        "    self.layernorm2 = layers.LayerNormalization()\n",
        "    self.layernorm3 = layers.LayerNormalization()\n",
        "    self.supports_masking=True\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({'embed_dim': self.embed_dim, 'dense_dim': self.dense_dim, 'num_heads': self.num_heads})\n",
        "    return config\n",
        "\n",
        "  def get_casual_attention_mask(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "    j = tf.range(sequence_length)\n",
        "    mask = tf.cast(i >= j, dtype='int32')\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "  def call(self, inputs, encoder_outputs, mask=None):\n",
        "    casual_mask = self.get_casual_attention_mask(inputs)\n",
        "    if mask is not None:\n",
        "      padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype='int32')\n",
        "      padding_mask = tf.minimum(padding_mask, casual_mask)\n",
        "    attention_output_1 = self.attention1(query=inputs, value=inputs, key=inputs, attention_mask=casual_mask)\n",
        "    attention_output_1 = self.layernorm1(inputs + attention_output_1)\n",
        "    attention_output_2 = self.attention2(query=attention_output_1, value=encoder_outputs, key=encoder_outputs, attention_mask=padding_mask)\n",
        "    attention_output_2 = self.layernorm2(attention_output_1 + attention_output_2)\n",
        "    proj_output = self.denseProj(attention_output_2)\n",
        "    return self.layernorm3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "owvGFuBhj6ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 2\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) \n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDXWCU0FjQ5P",
        "outputId": "93962e3f-bee6-4817-fb55-4b1d20979bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " positional_embedding (Position  (None, None, 256)   3865600     ['input_1[0][0]']                \n",
            " alEmbedding)                                                                                     \n",
            "                                                                                                  \n",
            " transformer_decoder (Transform  (None, None, 256)   2104576     ['positional_embedding[0][0]',   \n",
            " erDecoder)                                                       'positional_embedding[0][0]']   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, None, 15000)  3855000     ['transformer_decoder[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 9,825,176\n",
            "Trainable params: 9,825,176\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))  #maps word indices back to strings, to be used for text decoding\n",
        "\n",
        "def sample_next(predictions, temperature=1.0):      #implementing temperature\n",
        " predictions = np.asarray(predictions).astype(\"float64\")\n",
        " predictions = np.log(predictions) / temperature\n",
        " exp_preds = np.exp(predictions)\n",
        " predictions = exp_preds / np.sum(exp_preds)\n",
        " probas = np.random.multinomial(1, predictions, 1)\n",
        " return np.argmax(probas)\n",
        "\n",
        "\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        " def __init__(self, prompt, generate_length, model_input_length, temperatures=(1.,), print_freq=1):\n",
        "  self.prompt = prompt        #In our case, prompt is \"this movie\"\n",
        "  self.generate_length = generate_length    #Length of generated output\n",
        "  self.model_input_length = model_input_length\n",
        "  self.temperatures = temperatures   #range of temperatures \n",
        "  self.print_freq = print_freq\n",
        "\n",
        " def on_epoch_end(self, epoch, logs=None):\n",
        "  if (epoch + 1) % self.print_freq != 0:\n",
        "    return\n",
        "  for temperature in self.temperatures:\n",
        "    print(\"== Generating with temperature\", temperature)\n",
        "    sentence = self.prompt \n",
        "    for i in range(self.generate_length):\n",
        "      tokenized_sentence = text_vectorization([sentence])    #Feed current sequence\n",
        "      predictions = self.model(tokenized_sentence)           #into our model\n",
        "      next_token = sample_next(predictions[0, i, :])      #Retrieve predictions for last timestep\n",
        "      sampled_token = tokens_index[next_token]            #and use them to sample a new word\n",
        "      sentence += \" \" + sampled_token       #append new word to current sequence and repeat\n",
        "    print(sentence)\n",
        "\n",
        "prompt = \"This movie\"\n",
        "text_gen_callback = TextGenerator(\n",
        " prompt,\n",
        " generate_length=50,\n",
        " model_input_length=sequence_length,\n",
        " temperatures=(0.2, 0.5, 0.7, 1., 1.5)) "
      ],
      "metadata": {
        "id": "udHSbQdqkPjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(lm_dataset, epochs=5, callbacks=[text_gen_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAA5Z2UHnqLt",
        "outputId": "260a993c-1b5f-40cd-ea6d-1bd0019ec23b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.3586== Generating with temperature 0.2\n",
            "This movie was is about a here [UNK] a [UNK] book [UNK] called like powers a we series were [UNK] going creatures to in take following place 1994 movies a not block as the white band creaky gave era nothing or new with version a shooting plot characters when are the pretty\n",
            "== Generating with temperature 0.5\n",
            "This movie is is simply true a the good destruction although keen it on a the [UNK] released of a story [UNK] ive a been few closer years to ago appreciate but the after patience hearing hired how for much lou like finds that that have alist been director able james to\n",
            "== Generating with temperature 0.7\n",
            "This movie is is [UNK] a young pathetic guy look like but strange they go do see love it accept out some and [UNK] everything through to laughing be through reduced it captive and that lends sequels a are morgue an and [UNK] feel cons the cheese lamest than right youve the\n",
            "== Generating with temperature 1.0\n",
            "This movie is is made best at known first cliched it [UNK] short michael [UNK] [UNK] and and ginger [UNK] rogers marie are [UNK] unknown dangerous and detective the still cue very humanity well watch star it killer 1950s company both on by screen stephen and king conventional and action [UNK] production\n",
            "== Generating with temperature 1.5\n",
            "This movie has can victory most of important mind comments that to these watch films as is much about of look joseph at dacascos the or tribes 5 the out unpredictable why and the the sea battle are sequences played are on significant so youd frequent remember calls one oh of that\n",
            "391/391 [==============================] - 174s 426ms/step - loss: 5.3586\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.8166== Generating with temperature 0.2\n",
            "This movie movies is will one be an [UNK] early film film my primarily time father its and a omg beautiful ten family joke has ive showed ever perfection hosted the by special b effects this as movie the was director possible and to to feel explain exactly our what good of\n",
            "== Generating with temperature 0.5\n",
            "This movie endearing is young cute [UNK] and playing gritty the apartment drag lady game to in perform this alike pathetic and piece alluring of soldiers highly of informed depth destruction at and heart doesnt and take not an to honest their title weight she since plays she [UNK] tries with hard\n",
            "== Generating with temperature 0.7\n",
            "This movie is from one my of favorite the parts many of people the appears paper that thin thoroughly khan cheerful was rose burt satan lancaster in or comedies the speaks first dramatic half edwards with neglected any direction of can bmovies still its put a into short a product dark without\n",
            "== Generating with temperature 1.0\n",
            "This movie film is starts set out with as just long but term still dr left watson right showing down how to all bring the their family challenges wondering drugs what are makes going really valiant shame resolution where how do jim this [UNK] [UNK] reminded goal me more and famous better\n",
            "== Generating with temperature 1.5\n",
            "This movie show is is a bad tv judgment movie dance has and every programme force for of me the i blues thought brothers its were so the bad opposite guys the so [UNK] i that just maybe got an naked discussion boy which and keeps i changing dont its dance a\n",
            "391/391 [==============================] - 169s 432ms/step - loss: 4.8166\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.5940== Generating with temperature 0.2\n",
            "This movie is is so a bad big movie watch it i and was back totally especially awesome by i the promise frequency i of dont the always producers laugh it about at the its part a is few now things have that never says seen an okay endless movie images something\n",
            "== Generating with temperature 0.5\n",
            "This movie movie is is very a nice great at very times bad but and just it to is end all just the amazing way david through lovers the of end cinema important touched subject us for a the movie film there is was a a complete wonderfully waste complained  that\n",
            "== Generating with temperature 0.7\n",
            "This movie 2003 really is a awful simple just true like to home what that a forgettable south movie armageddon its im movie not went sure into what it it always was got great me with and almost b every and word i that [UNK] we khan are was still outstanding a\n",
            "== Generating with temperature 1.0\n",
            "This movie is was simply an a [UNK] real saying story about would the have portrayal died of in four it times was is a possible [UNK] bakshi of i my caught father the perhaps prime because example what my plot father was seemed born like however description the the next film\n",
            "== Generating with temperature 1.5\n",
            "This movie film could it have be in rather itself than as a other teenager film it as has it several has critics been like [UNK] all [UNK] the and subject know matter true its and [UNK] fail this to is sit the through movie the never films tire [UNK] of [UNK]\n",
            "391/391 [==============================] - 169s 432ms/step - loss: 4.5940\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.4681== Generating with temperature 0.2\n",
            "This movie movie is is so about blatantly every borders character of sheet selfabsorbed he jersey has shooting devised at a noname par time with [UNK] [UNK] maya longtime voyage onscreen [UNK] which never is will being save an for overall your presentation amateur  most zombie shocks of double presents a\n",
            "== Generating with temperature 0.5\n",
            "This movie is was a total real waste piece of of my [UNK] money edition so ive far [UNK] thought manos that frankie way [UNK] from was home carter movies miserably are seriously some corporate of cinema the fanatics made would it never better leave on me an i idea ever of\n",
            "== Generating with temperature 0.7\n",
            "This movie movie is really made bad for in using the accents generally or outright movement cheap in [UNK] churned some out of such caricature movies doesnt as have americas the [UNK] strangeness process or quality crimes is against only freedom deals fighters michele with [UNK] a [UNK] large [UNK] colonial grade\n",
            "== Generating with temperature 1.0\n",
            "This movie film is is not just rushed like in joshua a subtitles psycho used but to art it this ought afternoon to the be battlefield proud earth of vs american evil blockbusters of like the us japanese first never three know remakes the of same bmovies hollywood used classic in shows\n",
            "== Generating with temperature 1.5\n",
            "This movie is turned off out by to the be fire bad pleasing ok then the you leading have lady even plenty though of maybe coffee the 1988 guys but will the love blood opera of om scum [UNK] however roller to on be the the critters fact ive that discovered they\n",
            "391/391 [==============================] - 169s 431ms/step - loss: 4.4681\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.3815== Generating with temperature 0.2\n",
            "This movie film made is under so script bad which its i gore dont runs think from so the long waste shots of atrocious most acting of hour its costumes really top too of bad old to story put lines this and film sound out like in watching order the to old\n",
            "== Generating with temperature 0.5\n",
            "This movie movie is is really very ridiculous interesting that even i michael love anderson story and about feeling half much of better the by cast your this friends movie i on nearly our every hero prior but to my this point movie it is first one to of go all to\n",
            "== Generating with temperature 0.7\n",
            "This movie poorly was [UNK] badly then scripted released the outside movie of houses different and houses the and actresses people home generally as dont they even perform know [UNK] but considering they anyone belong who to reads earth a beat name of like a the cameras man on alexander visuals a\n",
            "== Generating with temperature 1.0\n",
            "This movie film made was at extremely every uncomfortable little and scene up was were horse ridiculous was is a a joke [UNK] about with it color makes as [UNK] family fighting woman for staring having in a french survived played the worse freedom tortured was souls that the she film just\n",
            "== Generating with temperature 1.5\n",
            "This movie was made from in so the many preview years theater ago it it wasted was on the an acting all and time action i was saw all the the simple picture in was the not scenes very were well laughing chosen and [UNK] the because actor of plain the damn\n",
            "391/391 [==============================] - 169s 432ms/step - loss: 4.3815\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8a957ba2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}