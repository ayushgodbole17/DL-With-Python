{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5B8XIEp3_Zrn",
    "outputId": "998b5cf8-4daa-4edf-d3b0-39cbe8eac060"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\\n!unzip -q spa-eng.zip'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
    "!unzip -q spa-eng.zip'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MTw8YhnR_iI1"
   },
   "outputs": [],
   "source": [
    "text_file = r\"path to spa.txt\"\n",
    "with open(text_file, encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = [] \n",
    "\n",
    "for line in lines: \n",
    "    english, spanish = line.split(\"\\t\") \n",
    "    spanish = \"[start] \" + spanish + \" [end]\" \n",
    "    text_pairs.append((english, spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iO4cEio_tfh",
    "outputId": "859a34ba-cc23-4d16-8ef1-b068438c72bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You had better hurry. The train leaves at three.', '[start] Ser√≠a mejor que os dierais prisa, el tren sale a las tres. [end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ixTn9SFP_yNz"
   },
   "outputs": [],
   "source": [
    "#Splitting into train-test-val\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2*num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples: num_train_samples+num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples+num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Z5FYfK8xA3LW"
   },
   "outputs": [],
   "source": [
    "#Two different TextVectorization layers for English and Spanish; Spanish has the extra ulta ? to be stripped, \n",
    "#and we want to preserve [start] and [end] tokens that we've inserted\n",
    "\n",
    "#Because toy example, we're stripping all punctuation; otherwise, we would tokenise them too so that generated\n",
    "#output has correct punctuation\n",
    "\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "\n",
    "#Find a way to get ulta question mark\n",
    "#strip_chars = string.punctuation + '?'\n",
    "\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace('[', '')\n",
    "strip_chars = strip_chars.replace(']', '')\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "  lowercase = tf.strings.lower(input_string)\n",
    "  return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "P0MItk4sC9Mo"
   },
   "outputs": [],
   "source": [
    "vocab_size=15000\n",
    "sequence_length=20\n",
    "\n",
    "from tensorflow.keras import layers as layers\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "source_vectorization = layers.TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)\n",
    "target_vectorization = layers.TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length+1,\n",
    "                                                standardize=custom_standardization)\n",
    "\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ki6LZu35D_Kd"
   },
   "outputs": [],
   "source": [
    "#Preparing datasets for translation\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "  eng = source_vectorization(eng)\n",
    "  spa = target_vectorization(spa)\n",
    "  return ({'english': eng, 'spanish':spa[:, :-1]}, spa[:, 1:])\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "  eng_texts, spa_texts = zip(*pairs)\n",
    "  eng_texts, spa_texts = list(eng_texts), list(spa_texts)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "  return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jJZAKmvFzj6",
    "outputId": "c8e3af1f-92ae-4bc8-eae6-329a078eadc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "  print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "  print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ueb6Ugo_F75h",
    "outputId": "834ac1a8-08e3-4934-87e7-392aec18b0ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 20, 128)           1920000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 20, 32)            20608     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20, 15000)         495000    \n",
      "=================================================================\n",
      "Total params: 2,435,608\n",
      "Trainable params: 2,435,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Seq2Seq with RNN\n",
    "\n",
    "inputs = keras.Input(shape=(sequence_length, ), dtype='int64')\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n",
    "x = layers.LSTM(32, return_sequences=True)(x)\n",
    "outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n",
    "\n",
    "#Just plain bad, we don't bother actually implementing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FCc2stY-HNi7"
   },
   "outputs": [],
   "source": [
    "#GRU-Based Encoder\n",
    "\n",
    "embed_dim=256\n",
    "latent_dim=1024\n",
    "\n",
    "source = keras.Input(shape=(None, ), dtype='int64', name='english')\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode='sum')(x)\n",
    "\n",
    "\n",
    "#GRU-Based Decoder\n",
    "\n",
    "past_target = keras.Input(shape=(None, ), dtype='int64', name='spanish')\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FC79Fsd6I-VD",
    "outputId": "2193a749-533e-400f-c022-e1962d99e0d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "english (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "spanish (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    3840000     english[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    3840000     spanish[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 1024)         7876608     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, None, 1024)   3938304     embedding_2[0][0]                \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 1024)   0           gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 15000)  15375000    dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 34,869,912\n",
      "Trainable params: 34,869,912\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_rnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "seq2seq_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "Bjq6nEKiJZHr",
    "outputId": "f0657f94-40a9-4d4b-e1cd-eb41845d27e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"callbacks = keras.callbacks.ModelCheckpoint('seq2seq_rnn.keras', save_best_only=True)\\nseq2seq_rnn.fit(train_ds, epochs=10, validation_data=val_ds, callbacks=callbacks)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''callbacks = keras.callbacks.ModelCheckpoint('seq2seq_rnn.keras', save_best_only=True)\n",
    "seq2seq_rnn.fit(train_ds, epochs=10, validation_data=val_ds, callbacks=callbacks)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nfiMbhpKJfAL"
   },
   "outputs": [],
   "source": [
    "#Using Transfomers\n",
    "\n",
    "#Implementing transformer encoder from scratch\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.embed_dim = embed_dim\n",
    "    self.dense_dim = dense_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    self.denseProj = keras.Sequential([layers.Dense(dense_dim, activation='relu'), layers.Dense(embed_dim), ])\n",
    "    self.layerNorm1 = layers.LayerNormalization()\n",
    "    self.layerNorm2 = layers.LayerNormalization()\n",
    "\n",
    "  def call(self, inputs, mask=None):\n",
    "    if mask is not None:\n",
    "      mask=mask[:, tf.newaxis, :]\n",
    "    attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "    proj_input = self.layerNorm1(inputs+attention_output)\n",
    "    proj_output = self.denseProj(proj_input)\n",
    "    return self.layerNorm2(proj_input + proj_output)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super().get_config()\n",
    "    config.update({'embed_dim':self.embed_dim, 'num_heads':self.num_heads, 'dense_dim': self.dense_dim})\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ndCn5ZxEVh8g"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "\n",
    " def __init__(self, sequence_length, input_dim, output_dim, **kwargs): \n",
    "  super().__init__(**kwargs)\n",
    "  self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
    "  self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim) \n",
    "  self.sequence_length = sequence_length\n",
    "  self.input_dim = input_dim\n",
    "  self.output_dim = output_dim\n",
    "\n",
    " def call(self, inputs):\n",
    "  length = tf.shape(inputs)[-1]\n",
    "  positions = tf.range(start=0, limit=length, delta=1)\n",
    "  embedded_tokens = self.token_embeddings(inputs)\n",
    "  embedded_positions = self.position_embeddings(positions)\n",
    "  return embedded_tokens + embedded_positions \n",
    "\n",
    " def compute_mask(self, inputs, mask=None): \n",
    "  return tf.math.not_equal(inputs, 0) \n",
    "\n",
    " def get_config(self): \n",
    "  config = super().get_config()\n",
    "  config.update({\"output_dim\": self.output_dim,\"sequence_length\": self.sequence_length, \"input_dim\": self.input_dim})\n",
    "  return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "I-UnFeJjVnRD"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.embed_dim = embed_dim\n",
    "    self.dense_dim = dense_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.attention1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    self.attention2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    self.denseProj = keras.Sequential([layers.Dense(dense_dim, activation='relu'), layers.Dense(embed_dim), ])\n",
    "    self.layernorm1 = layers.LayerNormalization()\n",
    "    self.layernorm2 = layers.LayerNormalization()\n",
    "    self.layernorm3 = layers.LayerNormalization()\n",
    "    self.supports_masking=True\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super().get_config()\n",
    "    config.update({'embed_dim': self.embed_dim, 'dense_dim': self.dense_dim, 'num_heads': self.num_heads})\n",
    "    return config\n",
    "\n",
    "  def get_casual_attention_mask(self, inputs):\n",
    "    input_shape = tf.shape(inputs)\n",
    "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "    j = tf.range(sequence_length)\n",
    "    mask = tf.cast(i >= j, dtype='int32')\n",
    "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "  def call(self, inputs, encoder_outputs, mask=None):\n",
    "    casual_mask = self.get_casual_attention_mask(inputs)\n",
    "    if mask is not None:\n",
    "      padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype='int32')\n",
    "      padding_mask = tf.minimum(padding_mask, casual_mask)\n",
    "    attention_output_1 = self.attention1(query=inputs, value=inputs, key=inputs, attention_mask=casual_mask)\n",
    "    attention_output_1 = self.layernorm1(inputs + attention_output_1)\n",
    "    attention_output_2 = self.attention2(query=attention_output_1, value=encoder_outputs, key=encoder_outputs, attention_mask=padding_mask)\n",
    "    attention_output_2 = self.layernorm2(attention_output_1 + attention_output_2)\n",
    "    proj_output = self.denseProj(attention_output_2)\n",
    "    return self.layernorm3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SmFyk36uVuKN"
   },
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "sequence_length = 600\n",
    "\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None, ), dtype='int64', name='english')\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None, ), dtype='int64', name='spanish')\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs) \n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) \n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLBXoSRxYigO",
    "outputId": "6f10e3ae-eecb-444a-abf4-2eba20012101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "english (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "spanish (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positional_embedding (Positiona (None, None, 256)    3993600     english[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "positional_embedding_1 (Positio (None, None, 256)    3993600     spanish[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder (Transforme (None, None, 256)    3155456     positional_embedding[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "transformer_decoder (Transforme (None, None, 256)    5259520     positional_embedding_1[0][0]     \n",
      "                                                                 transformer_encoder[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 256)    0           transformer_decoder[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, None, 15000)  3855000     dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 20,257,176\n",
      "Trainable params: 20,257,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23OHbQnZYkIS",
    "outputId": "52c301d9-601e-4a65-b340-9638951711ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1302/1302 [==============================] - 759s 581ms/step - loss: 1.3927 - accuracy: 0.4764 - val_loss: 0.9527 - val_accuracy: 0.5931\n",
      "Epoch 2/25\n",
      "1302/1302 [==============================] - 754s 579ms/step - loss: 0.8830 - accuracy: 0.6202 - val_loss: 0.7578 - val_accuracy: 0.6561\n",
      "Epoch 3/25\n",
      "1302/1302 [==============================] - 754s 579ms/step - loss: 0.6975 - accuracy: 0.6744 - val_loss: 0.6934 - val_accuracy: 0.6780\n",
      "Epoch 4/25\n",
      "1302/1302 [==============================] - 763s 586ms/step - loss: 0.5886 - accuracy: 0.7080 - val_loss: 0.6658 - val_accuracy: 0.6901\n",
      "Epoch 5/25\n",
      "1302/1302 [==============================] - 753s 579ms/step - loss: 0.5146 - accuracy: 0.7322 - val_loss: 0.6506 - val_accuracy: 0.6975\n",
      "Epoch 6/25\n",
      "1302/1302 [==============================] - 755s 580ms/step - loss: 0.4597 - accuracy: 0.7523 - val_loss: 0.6513 - val_accuracy: 0.7026\n",
      "Epoch 7/25\n",
      "1302/1302 [==============================] - 755s 580ms/step - loss: 0.4175 - accuracy: 0.7688 - val_loss: 0.6522 - val_accuracy: 0.7019\n",
      "Epoch 8/25\n",
      "1302/1302 [==============================] - 755s 580ms/step - loss: 0.3830 - accuracy: 0.7826 - val_loss: 0.6620 - val_accuracy: 0.7048\n",
      "Epoch 9/25\n",
      "1302/1302 [==============================] - 754s 579ms/step - loss: 0.3553 - accuracy: 0.7949 - val_loss: 0.6681 - val_accuracy: 0.7066\n",
      "Epoch 10/25\n",
      "1302/1302 [==============================] - 754s 579ms/step - loss: 0.3315 - accuracy: 0.8053 - val_loss: 0.6858 - val_accuracy: 0.7048\n",
      "Epoch 11/25\n",
      "1302/1302 [==============================] - 755s 580ms/step - loss: 0.3105 - accuracy: 0.8149 - val_loss: 0.7003 - val_accuracy: 0.7063\n",
      "Epoch 12/25\n",
      "1302/1302 [==============================] - 754s 579ms/step - loss: 0.2929 - accuracy: 0.8229 - val_loss: 0.7009 - val_accuracy: 0.7078\n",
      "Epoch 13/25\n",
      "1302/1302 [==============================] - 754s 579ms/step - loss: 0.2781 - accuracy: 0.8298 - val_loss: 0.7173 - val_accuracy: 0.7065\n",
      "Epoch 14/25\n",
      "1302/1302 [==============================] - 755s 580ms/step - loss: 0.2632 - accuracy: 0.8376 - val_loss: 0.7272 - val_accuracy: 0.7086\n",
      "Epoch 15/25\n",
      "1302/1302 [==============================] - 753s 579ms/step - loss: 0.2519 - accuracy: 0.8432 - val_loss: 0.7460 - val_accuracy: 0.7086\n",
      "Epoch 16/25\n",
      "1302/1302 [==============================] - 755s 580ms/step - loss: 0.2393 - accuracy: 0.8494 - val_loss: 0.7432 - val_accuracy: 0.7074\n",
      "Epoch 17/25\n",
      "1302/1302 [==============================] - 754s 579ms/step - loss: 0.2292 - accuracy: 0.8550 - val_loss: 0.7559 - val_accuracy: 0.7084\n",
      "Epoch 18/25\n",
      "1302/1302 [==============================] - 755s 580ms/step - loss: 0.2191 - accuracy: 0.8601 - val_loss: 0.7714 - val_accuracy: 0.7096\n",
      "Epoch 19/25\n",
      "1302/1302 [==============================] - 754s 579ms/step - loss: 0.2100 - accuracy: 0.8650 - val_loss: 0.7827 - val_accuracy: 0.7093\n",
      "Epoch 20/25\n",
      "1302/1302 [==============================] - 753s 579ms/step - loss: 0.2011 - accuracy: 0.8697 - val_loss: 0.7920 - val_accuracy: 0.7103\n",
      "Epoch 21/25\n",
      "1302/1302 [==============================] - 750s 576ms/step - loss: 0.1939 - accuracy: 0.8739 - val_loss: 0.8064 - val_accuracy: 0.7104\n",
      "Epoch 22/25\n",
      "1302/1302 [==============================] - 756s 581ms/step - loss: 0.1863 - accuracy: 0.8780 - val_loss: 0.8160 - val_accuracy: 0.7109\n",
      "Epoch 23/25\n",
      "1302/1302 [==============================] - 753s 579ms/step - loss: 0.1793 - accuracy: 0.8816 - val_loss: 0.8229 - val_accuracy: 0.7109\n",
      "Epoch 24/25\n",
      "1302/1302 [==============================] - 754s 579ms/step - loss: 0.1738 - accuracy: 0.8847 - val_loss: 0.8242 - val_accuracy: 0.7109\n",
      "Epoch 25/25\n",
      "1302/1302 [==============================] - 753s 579ms/step - loss: 0.1681 - accuracy: 0.8882 - val_loss: 0.8433 - val_accuracy: 0.7091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19123051490>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "callbacks = keras.callbacks.ModelCheckpoint('transformer.keras', save_best_only=True)\n",
    "transformer.fit(train_ds, epochs=25, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgHYe79JY05H",
    "outputId": "05aed135-b93a-4878-fb11-15117d518fd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Tom has been waiting three hours now.\n",
      "[start] tom ha estado esperando tres horas de ahora [end]\n",
      "-\n",
      "I was married once.\n",
      "[start] estuve casado una vez [end]\n",
      "-\n",
      "I'm right, aren't I?\n",
      "[start] tengo raz√≥n yo ¬øverdad [end]\n",
      "-\n",
      "At the funeral, the widow looked very dignified, with her black suit, hat and gloves.\n",
      "[start] en el funeral la viuda de la f√°brica con una negro y el jersey amarillo [end]\n",
      "-\n",
      "The car's antenna is built into the windshield.\n",
      "[start] el coches registro es constru√≠ para la [UNK] [end]\n",
      "-\n",
      "He fell asleep behind the wheel and had an accident.\n",
      "[start] √âl se qued√≥ dormido cerca del volante hab√≠a accidente y un accidente [end]\n",
      "-\n",
      "I just hope nothing goes wrong this time.\n",
      "[start] solo espero que no pase nada en este momento [end]\n",
      "-\n",
      "It's not worth reading any further.\n",
      "[start] no vale la pena leer nada [end]\n",
      "-\n",
      "My nose itches.\n",
      "[start] me duele el [UNK] [end]\n",
      "-\n",
      "He will notice sooner or later.\n",
      "[start] √âl se [UNK] tarde o temprano [end]\n",
      "-\n",
      "I'll expect you at 2:30.\n",
      "[start] te espero a las dos y media [end]\n",
      "-\n",
      "The climate's very mild.\n",
      "[start] la [UNK] muy suave [end]\n",
      "-\n",
      "We don't even know what it is.\n",
      "[start] ni siquiera sabemos qu√© es [end]\n",
      "-\n",
      "Share it with us.\n",
      "[start] [UNK] con nosotras [end]\n",
      "-\n",
      "He hesitated for a while.\n",
      "[start] √âl vacil√≥ por un rato [end]\n",
      "-\n",
      "Tom will have breakfast early tomorrow.\n",
      "[start] tom desayuna el desayuno temprano ma√±ana [end]\n",
      "-\n",
      "Children learn to respond to rhythmical sounds from a very young age.\n",
      "[start] los ni√±os odian a buena capa de [UNK] [end]\n",
      "-\n",
      "He is known to the entire country.\n",
      "[start] √âl es conocido por todo el pa√≠s [end]\n",
      "-\n",
      "We are hoping to visit Spain this summer.\n",
      "[start] esperamos que visitar espa√±a esta monta√±a [end]\n",
      "-\n",
      "I see now that we've made a mistake.\n",
      "[start] ahora veo que hemos cometido un error [end]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "def decode_sequence(input_sentence):\n",
    " tokenized_input_sentence = source_vectorization([input_sentence])\n",
    " decoded_sentence = \"[start]\"\n",
    " for i in range(max_decoded_sentence_length):\n",
    "  tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
    "  predictions = transformer([tokenized_input_sentence, tokenized_target_sentence]) \n",
    "  sampled_token_index = np.argmax(predictions[0, i, :]) \n",
    "  sampled_token = spa_index_lookup[sampled_token_index] \n",
    "  decoded_sentence += \" \" + sampled_token \n",
    "  if sampled_token == \"[end]\": \n",
    "   break \n",
    " return decoded_sentence\n",
    "test_eng_texts = [pair[0] for pair in test_pairs] \n",
    "for _ in range(20):\n",
    " input_sentence = random.choice(test_eng_texts)\n",
    " print(\"-\")\n",
    " print(input_sentence)\n",
    " print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9xPBC_bgJMe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "machine_translation_1.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
